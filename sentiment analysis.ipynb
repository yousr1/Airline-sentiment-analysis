{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('tweet_id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  airline_sentiment_confidence negativereason  \\\n",
       "0           neutral                        1.0000            NaN   \n",
       "1          positive                        0.3486            NaN   \n",
       "2           neutral                        0.6837            NaN   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason_confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "=================================================================================\n",
      "airline_sentiment_confidence\n",
      "1.0000    10445\n",
      "0.6667       71\n",
      "0.6632       35\n",
      "0.6559       30\n",
      "0.6596       30\n",
      "0.6701       29\n",
      "0.6598       28\n",
      "0.6600       26\n",
      "0.6842       26\n",
      "0.6771       24\n",
      "0.6739       22\n",
      "0.6737       22\n",
      "0.6630       22\n",
      "0.6421       22\n",
      "0.6809       22\n",
      "0.6774       22\n",
      "0.6735       21\n",
      "0.6522       20\n",
      "0.6566       19\n",
      "0.6633       19\n",
      "0.6703       19\n",
      "0.6563       18\n",
      "0.6768       17\n",
      "0.6742       17\n",
      "0.6634       17\n",
      "0.6593       17\n",
      "0.6889       16\n",
      "0.6591       16\n",
      "0.6832       16\n",
      "0.6804       15\n",
      "          ...  \n",
      "0.3537        1\n",
      "0.6958        1\n",
      "0.3708        1\n",
      "0.3533        1\n",
      "0.6373        1\n",
      "0.3666        1\n",
      "0.9286        1\n",
      "0.6155        1\n",
      "0.3606        1\n",
      "0.3661        1\n",
      "0.3486        1\n",
      "0.3626        1\n",
      "0.9220        1\n",
      "0.3612        1\n",
      "0.7145        1\n",
      "0.6102        1\n",
      "0.6284        1\n",
      "0.6159        1\n",
      "0.7134        1\n",
      "0.6486        1\n",
      "0.6586        1\n",
      "0.3493        1\n",
      "0.6355        1\n",
      "0.7171        1\n",
      "0.7119        1\n",
      "0.3913        1\n",
      "0.7273        1\n",
      "0.6353        1\n",
      "0.6260        1\n",
      "0.3544        1\n",
      "Name: airline_sentiment_confidence, Length: 1023, dtype: int64\n",
      "=================================================================================\n",
      "negativereason\n",
      "Customer Service Issue         2910\n",
      "Late Flight                    1665\n",
      "Can't Tell                     1190\n",
      "Cancelled Flight                847\n",
      "Lost Luggage                    724\n",
      "Bad Flight                      580\n",
      "Flight Booking Problems         529\n",
      "Flight Attendant Complaints     481\n",
      "longlines                       178\n",
      "Damaged Luggage                  74\n",
      "Name: negativereason, dtype: int64\n",
      "=================================================================================\n",
      "negativereason_confidence\n",
      "1.0000    3436\n",
      "0.0000    1344\n",
      "0.6667      62\n",
      "0.6632      33\n",
      "0.6596      29\n",
      "0.6733      25\n",
      "0.6809      25\n",
      "0.3441      24\n",
      "0.6559      24\n",
      "0.6701      23\n",
      "0.6526      22\n",
      "0.6842      21\n",
      "0.6702      21\n",
      "0.6600      21\n",
      "0.6737      20\n",
      "0.6598      20\n",
      "0.6771      19\n",
      "0.6838      18\n",
      "0.6495      18\n",
      "0.6806      18\n",
      "0.3474      18\n",
      "0.6593      18\n",
      "0.6633      18\n",
      "0.6606      17\n",
      "0.6522      17\n",
      "0.6630      17\n",
      "0.3684      17\n",
      "0.3579      17\n",
      "0.3469      17\n",
      "0.6563      17\n",
      "          ... \n",
      "0.3870       1\n",
      "0.3378       1\n",
      "0.3834       1\n",
      "0.3228       1\n",
      "0.3700       1\n",
      "0.6950       1\n",
      "0.7083       1\n",
      "0.6171       1\n",
      "0.3538       1\n",
      "0.5193       1\n",
      "0.3906       1\n",
      "0.6227       1\n",
      "0.3374       1\n",
      "0.3272       1\n",
      "0.6815       1\n",
      "0.3645       1\n",
      "0.9241       1\n",
      "0.3796       1\n",
      "0.3864       1\n",
      "0.7237       1\n",
      "0.6312       1\n",
      "0.3930       1\n",
      "0.3379       1\n",
      "0.6346       1\n",
      "0.3349       1\n",
      "0.3386       1\n",
      "0.3577       1\n",
      "0.3249       1\n",
      "0.3290       1\n",
      "0.3255       1\n",
      "Name: negativereason_confidence, Length: 1410, dtype: int64\n",
      "=================================================================================\n",
      "airline\n",
      "United            3822\n",
      "US Airways        2913\n",
      "American          2759\n",
      "Southwest         2420\n",
      "Delta             2222\n",
      "Virgin America     504\n",
      "Name: airline, dtype: int64\n",
      "=================================================================================\n",
      "airline_sentiment_gold\n",
      "negative    32\n",
      "positive     5\n",
      "neutral      3\n",
      "Name: airline_sentiment_gold, dtype: int64\n",
      "=================================================================================\n",
      "name\n",
      "JetBlueNews        63\n",
      "kbosspotter        32\n",
      "_mhertz            29\n",
      "otisday            28\n",
      "throthra           27\n",
      "weezerandburnie    23\n",
      "rossj987           23\n",
      "MeeestarCoke       22\n",
      "GREATNESSEOA       22\n",
      "scoobydoo9749      21\n",
      "jasemccarty        20\n",
      "flemmingerin       19\n",
      "ElmiraBudMan       19\n",
      "georgetietjen      19\n",
      "Aero0729           18\n",
      "ThatJasonEaton     18\n",
      "thomashoward88     18\n",
      "chagaga2013        18\n",
      "worldwideweg       17\n",
      "SMHillman          17\n",
      "heyheyman          16\n",
      "arthurhasher       16\n",
      "patrick_maness     16\n",
      "luvthispayne       15\n",
      "Allisonjones704    15\n",
      "ColtSTaylor        14\n",
      "riricesq           14\n",
      "geekstiel          14\n",
      "farfalla818        14\n",
      "Heavenlychc9       14\n",
      "                   ..\n",
      "artburkart          1\n",
      "LimeLiberator       1\n",
      "JacquieMae08        1\n",
      "skenniston          1\n",
      "Lulabella83         1\n",
      "jcfly123            1\n",
      "1_7_8_0             1\n",
      "followkashyap       1\n",
      "nerisa1127          1\n",
      "CoryDauphin         1\n",
      "Drzah_              1\n",
      "praywinn            1\n",
      "sammy_ike_blaze     1\n",
      "sdpurv              1\n",
      "drinkmud            1\n",
      "jmacie3             1\n",
      "meganpberger        1\n",
      "ashliewhite         1\n",
      "RosieRHues          1\n",
      "bgroberto3246       1\n",
      "bradtronic          1\n",
      "GottaGoFlying       1\n",
      "L_Hindle            1\n",
      "MarcPerez           1\n",
      "cabowine            1\n",
      "sgad1983            1\n",
      "SkyDiverChad        1\n",
      "AmaniHardrict       1\n",
      "dtndtndtndtn        1\n",
      "Yorkshire_D         1\n",
      "Name: name, Length: 7701, dtype: int64\n",
      "=================================================================================\n",
      "negativereason_gold\n",
      "Customer Service Issue                      12\n",
      "Late Flight                                  4\n",
      "Cancelled Flight                             3\n",
      "Can't Tell                                   3\n",
      "Cancelled Flight\\nCustomer Service Issue     2\n",
      "Flight Attendant Complaints                  1\n",
      "Late Flight\\nCancelled Flight                1\n",
      "Late Flight\\nLost Luggage                    1\n",
      "Bad Flight                                   1\n",
      "Customer Service Issue\\nLost Luggage         1\n",
      "Late Flight\\nFlight Attendant Complaints     1\n",
      "Lost Luggage\\nDamaged Luggage                1\n",
      "Customer Service Issue\\nCan't Tell           1\n",
      "Name: negativereason_gold, dtype: int64\n",
      "=================================================================================\n",
      "retweet_count\n",
      "0     13873\n",
      "1       640\n",
      "2        66\n",
      "3        22\n",
      "4        17\n",
      "5         5\n",
      "7         3\n",
      "6         3\n",
      "22        2\n",
      "8         1\n",
      "32        1\n",
      "9         1\n",
      "31        1\n",
      "18        1\n",
      "15        1\n",
      "28        1\n",
      "44        1\n",
      "11        1\n",
      "Name: retweet_count, dtype: int64\n",
      "=================================================================================\n",
      "text\n",
      "@united thanks                                                                                                                                            6\n",
      "@SouthwestAir sent                                                                                                                                        5\n",
      "@AmericanAir thanks                                                                                                                                       5\n",
      "@JetBlue thanks!                                                                                                                                          5\n",
      "@AmericanAir thank you!                                                                                                                                   4\n",
      "@united thank you!                                                                                                                                        4\n",
      "@SouthwestAir thank you!                                                                                                                                  3\n",
      "@USAirways YOU ARE THE BEST!!! FOLLOW ME PLEASE;)🙏🙏🙏✌️✌️✌️🙏🙏🙏                                                                                             3\n",
      "@united thank you                                                                                                                                         3\n",
      "@united Thanks!                                                                                                                                           3\n",
      "@AmericanAir thanks!                                                                                                                                      3\n",
      "@SouthwestAir Thank you!                                                                                                                                  3\n",
      "@JetBlue thank you!                                                                                                                                       3\n",
      "@USAirways thanks                                                                                                                                         3\n",
      "@USAirways thank you                                                                                                                                      3\n",
      "@AmericanAir I might look into that. My wife travels much more than I do. Could we both use the membership?                                               2\n",
      "@AmericanAir yes yes yes,so glad to be headed home!                                                                                                       2\n",
      "@AmericanAir I thought all those planes were retired? #MD80                                                                                               2\n",
      "@AmericanAir I'm frustrated by all of the @USAirways attitude toward #ExecPlat members. #thenewamerican                                                   2\n",
      "@AmericanAir thanks for getting back to me. But I will find other airlines in the future.                                                                 2\n",
      "@AmericanAir can u help rebook passenger via Twitter/DM.  Been on hold for 1.5 hours. Thanks!                                                             2\n",
      "@AmericanAir still waiting for a flight... I should get my money back                                                                                     2\n",
      "@AmericanAir how about some rampers at gate b40 dfw?   Waiting to be marshaled in                                                                         2\n",
      "@AmericanAir I tried to book a rwrd and was told I couldnt. Bought tix on USAir (now AA-no choice) didn't bother to + AAdv# with this svc...              2\n",
      "@AmericanAir I want to speak to a human being! !!  This is not an obscene request!                                                                        2\n",
      "@AmericanAir sorry so Late Flight, responded to your DM.                                                                                                  2\n",
      "@AmericanAir no response to DM or email yet.  customer service?                                                                                           2\n",
      "“@AmericanAir: @Andrew_Wasila We're sorry you were uncomfortable, Andrew. What can we do for you?” SMA                                                    2\n",
      "@AmericanAir 767 seconds from touchdown at Madrid airport in April 2013 #AvGeek http://t.co/1yWXRfn0Gr                                                    2\n",
      "@AmericanAir\\n Your response could have made all the difference. It could have made the situation better. NO TRUST...GET LOST like my bag.                2\n",
      "                                                                                                                                                         ..\n",
      "@AmericanAir the ticket is a poor gesture of goodwill (for missed trip) to a top status flyer - I may not next time (AA91 LHR-ORD 2/16)                   1\n",
      "@USAirways The automated message isn't helpful and it's impossible to speak with a human right now. Desperately need our luggage :(                       1\n",
      "@united Please send me the link/email to formally compliment Irene in SLC on some of the best customer service ever. #PaxEx                               1\n",
      "@SouthwestAir belt issues. Been a rough day for them. Worst part is traveling with 9 month old and having to wait outside in 85 degree                    1\n",
      "@JetBlue flight was Cancelled Flighted. Do I need to file a claim for the $50 credit I'm entitled to per yr bill of rights, or will it just be issued?    1\n",
      "@JetBlue A320 pulling into the gate as the sunrises here at @BostonLogan this morning #jetbluesofly #jetblue #airbus http://t.co/JGdu5us8Dz               1\n",
      "@JetBlue Airways Stock Rating Lowered by Vetr Inc. (JBLU) - Dakota Financial News http://t.co/QW2eBEEMVg                                                  1\n",
      "@USAirways he is 8 months old and is suffering from a rare form of cancer. Your customer service agents had no compassion and I am disgusted              1\n",
      "@AmericanAir first the pilot, then the catering...                                                                                                        1\n",
      "@united !!!!! YAAYY!!! YAY PROM!!!! http://t.co/DXicoyioxF                                                                                                1\n",
      "“@JetBlue: Our fleet's on fleek. http://t.co/MB0hVfqB1t”-see what ihop started 😑                                                                          1\n",
      "@JetBlue @NHLBruins repping the Bruins in Cleveland at Rock n Roll HOF &amp; The Christmas Story house #JetBlueBruins http://t.co/JKnWuGJ778              1\n",
      "@jetblue it's time for a direct flight from #JFK to #PITT.... @ Official JetBlue Terminal 5 - New York… http://t.co/QygXGmd3Sn                            1\n",
      "@united I did they did not.  I did submit a complaint on line and never heard back from that - I don't have that ID- you should find it                   1\n",
      "@usairways - It's all automated and no one cares to understand the problem I'm having                                                                     1\n",
      "@united didn't get her name. She was not in our group. She was sitting behind us. Think it was window seat #40? We only overheard...                      1\n",
      "@USAirways Cool. Now @SweetingR has been told you can't re-route his bags after he was told to leave them last night. He needs them to race.              1\n",
      "@united Is a snowboard boot bag included in the standard checked baggage next to the snowboard bag?                                                       1\n",
      "@SouthwestAir sadly didn't get much help ... As a travel agent this is so disappointing to me.                                                            1\n",
      "@AmericanAir @Cowboycerrone  ratchet airway!. ......                                                                                                      1\n",
      "“@JetBlue: Our fleet's on fleek. http://t.co/LSYbgLF59j” let's keep it professional                                                                       1\n",
      "@JetBlue I did get the email. Thought i wasn't supposed to reply to those😂                                                                                1\n",
      "@SouthwestAir did anyone come into work today? u should have all the pilots and flight attendants answer phones if no one is flying                       1\n",
      "@united a report was filed with the airport police on 11th and 12th February-I have the police case number if required                                    1\n",
      "@united A measly $50 e-certificate is not how you appreciate loyal customers after they wait 3hrs on the tarmac during UA1116. #unacceptable              1\n",
      "@AmericanAir  Yes I do bit you don't follow me so I can't DM you                                                                                          1\n",
      "@united thanks for a terrible experience at EWR. Customers should be treated as a priority, not as an inconvenience #tryagain                             1\n",
      "@united Also, group 5 is total BS.                                                                                                                        1\n",
      "@AmericanAir even with calls you haven't been able to help us anyway. #nevergettinghome                                                                   1\n",
      "@USAirways flt 5302 CLT to DAY supposed to depart 5:51; the 6:20...still no crew. #schedule/contact the pilot!                                            1\n",
      "Name: text, Length: 14427, dtype: int64\n",
      "=================================================================================\n",
      "tweet_coord\n",
      "[0.0, 0.0]                      164\n",
      "[40.64656067, -73.78334045]       6\n",
      "[32.91792297, -97.00367737]       3\n",
      "[40.64646912, -73.79133606]       3\n",
      "[37.62006843, -122.38822083]      2\n",
      "[34.0213466, -118.45229268]       2\n",
      "[39.83426941, -104.69960636]      2\n",
      "[35.22643463, -80.93879965]       2\n",
      "[40.69017276, -73.91646118]       2\n",
      "[32.82813261, -97.25115941]       2\n",
      "[18.22245647, -63.00369733]       2\n",
      "[37.78618135, -122.45742542]      2\n",
      "[39.1766101, -76.6700606]         2\n",
      "[40.69002464, -73.91638072]       2\n",
      "[40.68996177, -73.91640136]       2\n",
      "[40.68994668, -73.91637642]       2\n",
      "[33.75348859, -116.36209633]      2\n",
      "[37.99311597, -84.52114659]       2\n",
      "[33.75539049, -116.36196163]      2\n",
      "[41.16012493, -81.39760735]       1\n",
      "[41.86591215, -87.6231126]        1\n",
      "[25.79939784, -80.27038889]       1\n",
      "[37.78194753, -122.43412918]      1\n",
      "[28.35144756, -81.54358392]       1\n",
      "[27.97628758, -82.53727303]       1\n",
      "[42.5696777, -71.42056878]        1\n",
      "[34.06143413, -118.39787418]      1\n",
      "[41.77838296, -87.7409109]        1\n",
      "[39.74529398, -104.99225976]      1\n",
      "[32.92215312, -96.97563515]       1\n",
      "                               ... \n",
      "[37.6208766, -122.3867929]        1\n",
      "[37.93882337, -107.82065521]      1\n",
      "[29.65283375, -95.275749]         1\n",
      "[38.5369071, -106.9349375]        1\n",
      "[37.93663309, -107.81820588]      1\n",
      "[32.85744983, -97.03548311]       1\n",
      "[40.6867848, -74.18351089]        1\n",
      "[29.98788997, -95.34470061]       1\n",
      "[40.68545235, -74.18374208]       1\n",
      "[51.322819, 5.3576218]            1\n",
      "[37.93644673, -107.81832713]      1\n",
      "[41.49090693, -81.71627307]       1\n",
      "[33.43588637, -111.99965664]      1\n",
      "[33.75538818, -116.36197002]      1\n",
      "[40.64638899, -73.78399051]       1\n",
      "[37.79374402, -122.39327564]      1\n",
      "[40.80718573, -73.95477259]       1\n",
      "[18.22245747, -63.00369895]       1\n",
      "[35.22287836, -80.94056653]       1\n",
      "[29.99135326, -95.52122091]       1\n",
      "[38.9470418, -77.4511456]         1\n",
      "[28.02034806, -82.52234293]       1\n",
      "[32.9070889, -97.03785947]        1\n",
      "[35.21905941, -80.9426995]        1\n",
      "[39.85871934, -104.67371484]      1\n",
      "[38.95804042, -77.26190063]       1\n",
      "[30.26196639, -97.75945775]       1\n",
      "[31.27777976, 121.59340615]       1\n",
      "[43.3230125, -73.64314219]        1\n",
      "[29.99653598, -95.53874781]       1\n",
      "Name: tweet_coord, Length: 832, dtype: int64\n",
      "=================================================================================\n",
      "tweet_created\n",
      "2015-02-24 09:54:34 -0800    5\n",
      "2015-02-24 11:43:05 -0800    4\n",
      "2015-02-24 11:32:49 -0800    3\n",
      "2015-02-24 09:14:01 -0800    3\n",
      "2015-02-24 10:01:50 -0800    3\n",
      "2015-02-24 11:38:11 -0800    3\n",
      "2015-02-23 10:58:58 -0800    3\n",
      "2015-02-23 06:57:24 -0800    3\n",
      "2015-02-24 11:38:47 -0800    3\n",
      "2015-02-23 15:25:46 -0800    3\n",
      "2015-02-23 14:18:58 -0800    3\n",
      "2015-02-24 09:47:16 -0800    2\n",
      "2015-02-21 16:31:13 -0800    2\n",
      "2015-02-24 09:03:57 -0800    2\n",
      "2015-02-24 11:04:01 -0800    2\n",
      "2015-02-23 11:24:31 -0800    2\n",
      "2015-02-24 10:22:49 -0800    2\n",
      "2015-02-23 11:27:25 -0800    2\n",
      "2015-02-22 14:22:00 -0800    2\n",
      "2015-02-24 09:33:08 -0800    2\n",
      "2015-02-24 09:21:10 -0800    2\n",
      "2015-02-20 16:10:03 -0800    2\n",
      "2015-02-22 17:14:56 -0800    2\n",
      "2015-02-24 09:19:51 -0800    2\n",
      "2015-02-23 11:01:39 -0800    2\n",
      "2015-02-24 09:10:50 -0800    2\n",
      "2015-02-18 11:05:45 -0800    2\n",
      "2015-02-23 06:43:21 -0800    2\n",
      "2015-02-24 11:32:18 -0800    2\n",
      "2015-02-24 11:43:32 -0800    2\n",
      "                            ..\n",
      "2015-02-24 11:36:12 -0800    1\n",
      "2015-02-18 07:31:37 -0800    1\n",
      "2015-02-19 20:53:16 -0800    1\n",
      "2015-02-23 17:53:55 -0800    1\n",
      "2015-02-22 13:08:28 -0800    1\n",
      "2015-02-18 07:18:37 -0800    1\n",
      "2015-02-22 16:30:04 -0800    1\n",
      "2015-02-19 09:22:42 -0800    1\n",
      "2015-02-23 13:21:28 -0800    1\n",
      "2015-02-17 08:55:48 -0800    1\n",
      "2015-02-18 17:36:19 -0800    1\n",
      "2015-02-22 16:18:33 -0800    1\n",
      "2015-02-19 16:07:44 -0800    1\n",
      "2015-02-22 14:02:57 -0800    1\n",
      "2015-02-18 14:09:15 -0800    1\n",
      "2015-02-21 18:03:22 -0800    1\n",
      "2015-02-22 19:41:09 -0800    1\n",
      "2015-02-22 09:10:19 -0800    1\n",
      "2015-02-21 13:33:54 -0800    1\n",
      "2015-02-22 20:46:42 -0800    1\n",
      "2015-02-23 13:50:04 -0800    1\n",
      "2015-02-20 09:04:37 -0800    1\n",
      "2015-02-19 15:26:57 -0800    1\n",
      "2015-02-24 10:39:27 -0800    1\n",
      "2015-02-23 20:48:30 -0800    1\n",
      "2015-02-20 06:53:26 -0800    1\n",
      "2015-02-22 14:12:48 -0800    1\n",
      "2015-02-23 16:26:10 -0800    1\n",
      "2015-02-22 09:01:11 -0800    1\n",
      "2015-02-21 12:51:17 -0800    1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tweet_created, Length: 14247, dtype: int64\n",
      "=================================================================================\n",
      "tweet_location\n",
      "Boston, MA                        157\n",
      "New York, NY                      156\n",
      "Washington, DC                    150\n",
      "New York                          127\n",
      "USA                               126\n",
      "Chicago                           104\n",
      "New York City                      96\n",
      "Los Angeles, CA                    96\n",
      "NYC                                95\n",
      "San Francisco, CA                  91\n",
      "San Francisco                      86\n",
      "Chicago, IL                        81\n",
      "Brooklyn, NY                       66\n",
      "Austin, TX                         64\n",
      "Los Angeles                        64\n",
      "Washington, D.C.                   63\n",
      "Boston                             62\n",
      "Dallas, TX                         54\n",
      "Washington DC                      53\n",
      "Nashville, TN                      45\n",
      "NY                                 42\n",
      "Texas                              42\n",
      "Philadelphia, PA                   38\n",
      "San Diego                          38\n",
      "Denver, CO                         37\n",
      "Houston, TX                        35\n",
      "Seattle                            34\n",
      "Global                             34\n",
      "Logan International Airport        32\n",
      "New York, New York                 31\n",
      "                                 ... \n",
      "North Tonawanda                     1\n",
      "Crestwood                           1\n",
      "The Road to Damascus                1\n",
      "Washington, D.C                     1\n",
      "Scott Depot, WV                     1\n",
      "Vienna, Austria                     1\n",
      "east brunswick, nj                  1\n",
      "SF mostly, NYC & London often.      1\n",
      " Tally                              1\n",
      "Wiltshire, UK                       1\n",
      "Sunnyside, NY                       1\n",
      "DC|LA                               1\n",
      "Victoria, BC, Canada                1\n",
      "Salem, NH                           1\n",
      "Hingham, MA                         1\n",
      "Sacramento,California               1\n",
      " NJ USA                             1\n",
      "Austin, Texas                       1\n",
      "Where there is love..               1\n",
      "Earth                               1\n",
      "Happy Valley                        1\n",
      "Douglas, MA                         1\n",
      "St. Petersburg                      1\n",
      "The Hill!!!                         1\n",
      "Oakland by way of Chicago           1\n",
      "Alvechurch, Worcestershire, UK      1\n",
      "Tiffin, Ohio                        1\n",
      "Napa, CA                            1\n",
      "suburbs of Pittsburgh....           1\n",
      "Cambridge, Ma                       1\n",
      "Name: tweet_location, Length: 3081, dtype: int64\n",
      "=================================================================================\n",
      "user_timezone\n",
      "Eastern Time (US & Canada)     3744\n",
      "Central Time (US & Canada)     1931\n",
      "Pacific Time (US & Canada)     1208\n",
      "Quito                           738\n",
      "Atlantic Time (Canada)          497\n",
      "Mountain Time (US & Canada)     369\n",
      "Arizona                         229\n",
      "London                          195\n",
      "Alaska                          108\n",
      "Sydney                          107\n",
      "Hawaii                          104\n",
      "Amsterdam                        74\n",
      "America/Chicago                  37\n",
      "America/New_York                 26\n",
      "Indiana (East)                   26\n",
      "Paris                            25\n",
      "Abu Dhabi                        23\n",
      "Brasilia                         23\n",
      "Dublin                           17\n",
      "Tehran                           17\n",
      "Santiago                         17\n",
      "Greenland                        17\n",
      "Athens                           16\n",
      "America/Los_Angeles              15\n",
      "Casablanca                       15\n",
      "New Delhi                        15\n",
      "Mid-Atlantic                     15\n",
      "Buenos Aires                     14\n",
      "Central America                  13\n",
      "Beijing                          11\n",
      "                               ... \n",
      "Singapore                         2\n",
      "Nairobi                           2\n",
      "Guam                              2\n",
      "Islamabad                         2\n",
      "Copenhagen                        2\n",
      "Hong Kong                         2\n",
      "Perth                             2\n",
      "Lima                              2\n",
      "Tokyo                             1\n",
      "America/Detroit                   1\n",
      "Monterrey                         1\n",
      "Wellington                        1\n",
      "Solomon Is.                       1\n",
      "America/Atikokan                  1\n",
      "Bern                              1\n",
      "Istanbul                          1\n",
      "Newfoundland                      1\n",
      "Prague                            1\n",
      "Irkutsk                           1\n",
      "Midway Island                     1\n",
      "EST                               1\n",
      "Pretoria                          1\n",
      "West Central Africa               1\n",
      "Warsaw                            1\n",
      "Kuala Lumpur                      1\n",
      "Canberra                          1\n",
      "Sarajevo                          1\n",
      "Saskatchewan                      1\n",
      "Lisbon                            1\n",
      "Bucharest                         1\n",
      "Name: user_timezone, Length: 85, dtype: int64\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns :\n",
    "    print(col)\n",
    "    print(df[col].value_counts())\n",
    "    print('=================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes about the data :\n",
    "\n",
    "-- > out of 14,000 approximalty all of them expressed their sentiment , most are negative\n",
    "\n",
    "-- > 10,000 where confident about their sentiment \n",
    "\n",
    "-- > for each negative sentiment there is a reason and a confidence in that reason 3000 , out of 9000 where 100% confident of their reasoning\n",
    "\n",
    "-- > airlines share of sentiments is equally divided , but virgin america have the thinest share \n",
    "\n",
    "-- > 40 gold_cunstomers -- with their reasons for negativity \n",
    "\n",
    "-- > same customers tweeted their sentiment several times \n",
    "\n",
    "-- > retweet count of a tweet gives an indication of it's impact ( whether it is main stream opinion , made in time of crisis or holidays)\n",
    "\n",
    "--- > there are duplicated text , cleaning removing the tags \n",
    "\n",
    "-- > the tweet_cord t0 be [0,0] can be indication of error , check tweet location for each , tweet locations need to be cleaned \n",
    "overall \n",
    "\n",
    "-- > tweets are created within febraury of 2015 ( is that month differ in airflights overall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline_sentiment\n",
      "0\n",
      "=============================================================================\n",
      "airline_sentiment_confidence\n",
      "0\n",
      "=============================================================================\n",
      "negativereason\n",
      "5462\n",
      "=============================================================================\n",
      "negativereason_confidence\n",
      "4118\n",
      "=============================================================================\n",
      "airline\n",
      "0\n",
      "=============================================================================\n",
      "airline_sentiment_gold\n",
      "14600\n",
      "=============================================================================\n",
      "name\n",
      "0\n",
      "=============================================================================\n",
      "negativereason_gold\n",
      "14608\n",
      "=============================================================================\n",
      "retweet_count\n",
      "0\n",
      "=============================================================================\n",
      "text\n",
      "0\n",
      "=============================================================================\n",
      "tweet_coord\n",
      "13621\n",
      "=============================================================================\n",
      "tweet_created\n",
      "0\n",
      "=============================================================================\n",
      "tweet_location\n",
      "4733\n",
      "=============================================================================\n",
      "user_timezone\n",
      "4820\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in df.columns :\n",
    "    print(col)\n",
    "    print(df[col].isna().sum())\n",
    "    print('=============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['airline_sentiment_gold'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['airline_sentiment_gold','negativereason_gold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  airline_sentiment_confidence negativereason  \\\n",
       "0           neutral                        1.0000            NaN   \n",
       "1          positive                        0.3486            NaN   \n",
       "2           neutral                        0.6837            NaN   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason_confidence         airline        name  retweet_count  \\\n",
       "0                        NaN  Virgin America     cairdin              0   \n",
       "1                     0.0000  Virgin America    jnardino              0   \n",
       "2                        NaN  Virgin America  yvonnalynn              0   \n",
       "3                     0.7033  Virgin America    jnardino              0   \n",
       "4                     1.0000  Virgin America    jnardino              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[df['negativereason'].isna()]['negativereason']='Not stated'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negativereason'].fillna('Not stated',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['negativereason'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negativereason_confidence'].fillna(0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc=df[~(df['tweet_coord'].isna() & df['tweet_location'].isna() & df['user_timezone'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['tweet_coord','tweet_location','user_timezone'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  airline_sentiment_confidence negativereason  \\\n",
       "0           neutral                        1.0000     Not stated   \n",
       "1          positive                        0.3486     Not stated   \n",
       "2           neutral                        0.6837     Not stated   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason_confidence         airline        name  retweet_count  \\\n",
       "0                     0.0000  Virgin America     cairdin              0   \n",
       "1                     0.0000  Virgin America    jnardino              0   \n",
       "2                     0.0000  Virgin America  yvonnalynn              0   \n",
       "3                     0.7033  Virgin America    jnardino              0   \n",
       "4                     1.0000  Virgin America    jnardino              0   \n",
       "\n",
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials t...   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3  @VirginAmerica it's really aggressive to blast...   \n",
       "4  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "               tweet_created  \n",
       "0  2015-02-24 11:35:52 -0800  \n",
       "1  2015-02-24 11:15:59 -0800  \n",
       "2  2015-02-24 11:15:48 -0800  \n",
       "3  2015-02-24 11:15:36 -0800  \n",
       "4  2015-02-24 11:14:45 -0800  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.900012</td>\n",
       "      <td>0.458315</td>\n",
       "      <td>0.082671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.162977</td>\n",
       "      <td>0.401096</td>\n",
       "      <td>0.746672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_sentiment_confidence  negativereason_confidence  retweet_count\n",
       "count                  14600.000000               14600.000000   14600.000000\n",
       "mean                       0.900012                   0.458315       0.082671\n",
       "std                        0.162977                   0.401096       0.746672\n",
       "min                        0.335000                   0.000000       0.000000\n",
       "25%                        0.692300                   0.000000       0.000000\n",
       "50%                        1.000000                   0.624400       0.000000\n",
       "75%                        1.000000                   0.703025       0.000000\n",
       "max                        1.000000                   1.000000      44.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ❤️ flying @VirginAmerica. ☺️👍'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(row):\n",
    "        reg=re.compile(r'@[\\w]*')\n",
    "        \n",
    "        row=RE_EMOJI.sub(r'',row)\n",
    "        \n",
    "def extract_text(strr):\n",
    "    reg=re.compile(r'(@[\\w]*)|(http[s]?:\\/\\/(www\\.)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)')\n",
    "    emoji_list=[c for c in strr if c in emoji.UNICODE_EMOJI]\n",
    "    filtred  = [strr for strr in strr.split() if not any(i in strr for i in emoji_list)]\n",
    "    clean_text=' '.join(filtred)\n",
    "    clean_text=reg.sub(r'', clean_text)\n",
    "    return pd.Series([clean_text,emoji_list],index=['text', 'emojis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      I love this graphic.  .\n",
       "emojis                         []\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stry=' I love this graphic. http://t.co/UT5GrRwAaA @abcd.'\n",
    "\n",
    "extract_text(stry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['text','emojis']]=df['text'].apply(extract_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emojis']=df['emojis'].apply(lambda x :  x if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Really missed a prime opportunity for Men Without Hats parody, there. '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stry='I flying are bats'\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \n",
    "\n",
    "    tag =nltk.pos_tag([word])[0][1][0]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    get=tag_dict.get(tag, wordnet.NOUN)\n",
    "    return get\n",
    "\n",
    "\n",
    "\n",
    "def lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = (stopwords.words('english'))\n",
    "    stop_words.append('I')\n",
    "  \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    tokens_list=[ token for token in tokens if token not in stop_words]\n",
    "    return ([lemmatizer.lemmatize(w, get_wordnet_pos(w))  for w in tokens_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Really missed a prime opportunity for Men Without Hats parody, there. '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df['text'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14600, 11672)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(df['text_str'])\n",
    "vector = vectorizer.transform(df['text_str'])\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_w=df['text'].iloc[7]\n",
    "\n",
    "def word2str(words):\n",
    "    return ' '.join(word for word in words)\n",
    "word2str(list_w)\n",
    "\n",
    "df['text_str']=df['text'].apply(word2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = lambda symbol: 'X' if symbol==True else 'O' if symbol==False else ' '\n",
    "\n",
    "df['sentiment_encode']=df['airline_sentiment'].apply(lambda x : 2 if x =='positive' else 1 if x =='neutral' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text_str'], df['sentiment_encode'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11680,), (11680,), (2920,), (2920,))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape , y_train.shape , x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11680, 20000)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2920, 20000)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20000)\n",
    "# encode document\n",
    "train_features = vectorizer.transform(x_train)\n",
    "test_features= vectorizer.transform(x_test)\n",
    "# summarize encoded vector\n",
    "print(train_features.shape)\n",
    "print(test_features.toarray())\n",
    "\n",
    "print(test_features.shape)\n",
    "print(train_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers = [\n",
    "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=200),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\data\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegressionis 0.6376712328767123\n",
      "Accuracy of KNeighborsClassifieris 0.6965753424657535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\data\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVCis 0.6376712328767123\n",
      "Accuracy of DecisionTreeClassifieris 0.711986301369863\n",
      "Accuracy of RandomForestClassifieris 0.760958904109589\n",
      "Accuracy of AdaBoostClassifieris 0.7027397260273973\n",
      "Accuracy of GaussianNBis 0.4469178082191781\n"
     ]
    }
   ],
   "source": [
    "dense_features=train_features.toarray()\n",
    "dense_test= test_features.toarray()\n",
    "Accuracy=[]\n",
    "Model=[]\n",
    "for classifier in Classifiers:\n",
    "    try:\n",
    "        fit = classifier.fit(train_features,y_train)\n",
    "        pred = fit.predict(test_features)\n",
    "        \n",
    "    except Exception:\n",
    "        fit = classifier.fit(dense_features,y_train)\n",
    "        pred = fit.predict(dense_test)\n",
    "    accuracy = accuracy_score(pred,y_test)\n",
    "    Accuracy.append(accuracy)\n",
    "    Model.append(classifier.__class__.__name__)\n",
    "    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#model =gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(reviews):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(TaggedDocument(v, [label]))\n",
    "    return labelized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = labelizeReviews(x_train, 'TRAIN')\n",
    "x_test = labelizeReviews(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df['text_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelizeReviews(data, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words='What say', tags=['Data_0']),\n",
       " TaggedDocument(words='plus add commercial experience tacky', tags=['Data_1']),\n",
       " TaggedDocument(words='today Must mean need take another trip', tags=['Data_2']),\n",
       " TaggedDocument(words='really aggressive blast obnoxious entertainment guest face amp little recourse', tags=['Data_3']),\n",
       " TaggedDocument(words='really big bad thing', tags=['Data_4']),\n",
       " TaggedDocument(words='seriously would pay 30 flight seat play really bad thing fly VA', tags=['Data_5']),\n",
       " TaggedDocument(words='yes nearly every time fly VX ear worm go away', tags=['Data_6']),\n",
       " TaggedDocument(words='Really miss prime opportunity Men Without Hats parody', tags=['Data_7']),\n",
       " TaggedDocument(words='Well NOW DO D', tags=['Data_8']),\n",
       " TaggedDocument(words='amaze arrive hour early You good', tags=['Data_9'])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "size = 400\n",
    "#instantiate our DM and DBOW models\n",
    "\n",
    "model_dbow =Doc2Vec(min_count=1, window=10, vector_size=size, sample=1e-3, negative=5, dm=0, workers=3)\n",
    "\n",
    "#build vocab over all reviews\n",
    "\n",
    "model_dbow.build_vocab(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14600"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We pass through the data set multiple times, shuffling the training reviews each time to improve accuracy.\n",
    "\n",
    "all_train_reviews =np.asarray(x_train, dtype=object)\n",
    "\n",
    "for epoch in range(3):\n",
    "    \n",
    "    perm = np.random.permutation(np.arange(11680))\n",
    "    model_dbow.train(all_train_reviews[perm],total_examples=11680,epochs=1)\n",
    "    print('model_dbow.train', model_dbow.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get training set vectors from our models\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model[z.labels[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.concatenate(vecs)\n",
    "\n",
    "train_vecs_dbow = getVecs(model_dbow, x_train, size)\n",
    "\n",
    "#train over test set\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(x_test.shape[0])\n",
    "    model_dbow.train(x_test[perm])\n",
    "#Construct vectors for test reviews\n",
    "\n",
    "test_vecs_dbow = getVecs(model_dbow, x_test, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data]",
   "language": "python",
   "name": "conda-env-data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
